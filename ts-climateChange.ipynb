{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-and-visualing-the-dataset\" data-toc-modified-id=\"Load-and-visualing-the-dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load and visualing the dataset</a></span></li><li><span><a href=\"#Analyse-&amp;-Commentary-on-the-dataset\" data-toc-modified-id=\"Analyse-&amp;-Commentary-on-the-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Analyse &amp; Commentary on the dataset</a></span></li><li><span><a href=\"#Prediction-with-ARMA/ARIMA\" data-toc-modified-id=\"Prediction-with-ARMA/ARIMA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prediction with ARMA/ARIMA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-for-stationarity\" data-toc-modified-id=\"Test-for-stationarity-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Test for stationarity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gross-data-analysis\" data-toc-modified-id=\"Gross-data-analysis-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Gross data analysis</a></span></li><li><span><a href=\"#Taking-the-monthy-difference\" data-toc-modified-id=\"Taking-the-monthy-difference-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Taking the monthy difference</a></span></li></ul></li><li><span><a href=\"#Autocorrelation-(acf)-&amp;-Partial-autocorrelation-(pacf)\" data-toc-modified-id=\"Autocorrelation-(acf)-&amp;-Partial-autocorrelation-(pacf)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Autocorrelation (acf) &amp; Partial autocorrelation (pacf)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gross-data\" data-toc-modified-id=\"Gross-data-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Gross data</a></span></li><li><span><a href=\"#Diff-data\" data-toc-modified-id=\"Diff-data-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Diff data</a></span></li></ul></li><li><span><a href=\"#Build-ARIMA-Model\" data-toc-modified-id=\"Build-ARIMA-Model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Build ARIMA Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-a-ARMA(4,2)\" data-toc-modified-id=\"Build-a-ARMA(4,2)-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Build a ARMA(4,2)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prediction-on-full-dataset\" data-toc-modified-id=\"Prediction-on-full-dataset-3.3.1.1\"><span class=\"toc-item-num\">3.3.1.1&nbsp;&nbsp;</span>Prediction on full dataset</a></span></li><li><span><a href=\"#Back-to-level-data\" data-toc-modified-id=\"Back-to-level-data-3.3.1.2\"><span class=\"toc-item-num\">3.3.1.2&nbsp;&nbsp;</span>Back to level data</a></span></li><li><span><a href=\"#Performance-of-the-prediction\" data-toc-modified-id=\"Performance-of-the-prediction-3.3.1.3\"><span class=\"toc-item-num\">3.3.1.3&nbsp;&nbsp;</span>Performance of the prediction</a></span></li></ul></li></ul></li><li><span><a href=\"#Train-Test-Split-method-1\" data-toc-modified-id=\"Train-Test-Split-method-1-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Train Test Split method 1</a></span></li><li><span><a href=\"#Re-Build-ARIMA-model\" data-toc-modified-id=\"Re-Build-ARIMA-model-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Re-Build ARIMA model</a></span><ul class=\"toc-item\"><li><span><a href=\"#ACF-&amp;-PACF\" data-toc-modified-id=\"ACF-&amp;-PACF-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>ACF &amp; PACF</a></span></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Prediction</a></span></li><li><span><a href=\"#Performance\" data-toc-modified-id=\"Performance-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Performance</a></span></li></ul></li></ul></li><li><span><a href=\"#Train-Test-Split-method-2\" data-toc-modified-id=\"Train-Test-Split-method-2-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train Test Split method 2</a></span></li><li><span><a href=\"#Build-RNN-model\" data-toc-modified-id=\"Build-RNN-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Build RNN model</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-model\" data-toc-modified-id=\"The-model-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>The model</a></span></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Prediction</a></span></li><li><span><a href=\"#Performance-of-the-model\" data-toc-modified-id=\"Performance-of-the-model-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Performance of the model</a></span></li></ul></li><li><span><a href=\"#Build-LSTM-model\" data-toc-modified-id=\"Build-LSTM-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Build LSTM model</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-model\" data-toc-modified-id=\"The-model-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>The model</a></span></li><li><span><a href=\"#Prediction-on-y_Test\" data-toc-modified-id=\"Prediction-on-y_Test-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Prediction on y_Test</a></span></li><li><span><a href=\"#Performance-of-the-model\" data-toc-modified-id=\"Performance-of-the-model-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Performance of the model</a></span></li></ul></li><li><span><a href=\"#Global-Results\" data-toc-modified-id=\"Global-Results-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Global Results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief : https://github.com/JTreguer/ia-bdx-ts-project2/blob/master/README.md  \n",
    "Dataset on : https://data.world/environmentdata/climate-change-earth-surface  \n",
    "more on : http://berkeleyearth.lbl.gov/regions/global-land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the seed\n",
    "SEED = 42\n",
    "import keras\n",
    "keras.backend.clear_session()\n",
    "from numpy import random as np_random\n",
    "np_random.seed(SEED)\n",
    "from tensorflow import random as tf_random\n",
    "tf_random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and visualing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = pd.read_csv(\"GlobalLandTemperatures_GlobalTemperatures.csv\")\n",
    "df_init.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init['dt'] = pd.to_datetime(df_init['dt'])\n",
    "df_init.sort_values('dt', inplace=True)\n",
    "df_init.set_index('dt', inplace=True)\n",
    "\n",
    "print(df_init.shape)\n",
    "\n",
    "df_init.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_init_v1= df_init[\"LandAverageTemperature\"].copy()\n",
    "df_init_v1 = pd.DataFrame(df_init_v1)\n",
    "df_init_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init_v1b = df_init_v1.loc[\"1753-01-01\":\"2015-12-01\"].copy()\n",
    "df_init_v1b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init_v1b.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 200\n",
    "l = df_init_v1b.shape[0] \n",
    "df_init_v1b[l-nb:l].plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse & Commentary on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monthly time series\n",
    "- 3192 data initially, but presented 12 NaN\n",
    "- dropping NaN data, we have 3156 from January 1753 to December 2015 \n",
    "- Time series are with no trend\n",
    "- Time series seem presenting lots of seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with ARMA/ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/time-series-data-stationary-python/\n",
    "\n",
    "Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.  \n",
    "\n",
    "Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "\n",
    "p-value\n",
    "\n",
    "p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n",
    "\n",
    "\n",
    "ADF Statistic\n",
    "\n",
    "The more negative this statistic, the more likely we are to reject the null hypothesis (we have a stationary dataset).\n",
    "\n",
    "As part of the output, we get a look-up table to help determine the ADF statistic. We can see that our statistic value of -4 is less than the value of -3.449 at 1%.\n",
    "\n",
    "This suggests that we can reject the null hypothesis with a significance level of less than 1% (ADF is less than -3.449) (i.e. a low probability that the result is a statistical fluke).\n",
    "\n",
    "Rejecting the null hypothesis means that the process has no unit root, and in turn that the time series is stationary or does not have time-dependent structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=df_init_v1b.copy()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolmean0 = temp.rolling(20).mean()\n",
    "rolstd0 = temp.rolling(20).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "orig0 = plt.plot(temp, color='blue', label='Original')\n",
    "mean0 = plt.plot(rolmean0, color='red', label='Rolling Mean')\n",
    "std0 = plt.plot(rolstd0, color='black', label = 'Rolling Std Deviation')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.legend(loc='best')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result0 = seasonal_decompose(temp, model='additive')\n",
    "result0.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Dickey-Fuller test\n",
    "res_DF0 = adfuller(temp)\n",
    "print('ADF Statistic: %f' % res_DF0[0])\n",
    "print('p-value: %f' % res_DF0[1])\n",
    "print('Critical Values:')\n",
    "for key, value in res_DF0[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the monthy difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init_v1b['diff'] = df_init_v1b['LandAverageTemperature'].diff()\n",
    "df_init_v1b.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_init_v1b['diff'].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startValue = df_init_v1b['LandAverageTemperature'][\"1753-01-01\"]\n",
    "tempDiff=df_init_v1b['diff'].loc[\"1753-02-01\":\"2015-12-01\"]\n",
    "print(tempDiff.shape)\n",
    "print(startValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolmean = tempDiff.rolling(20).mean()\n",
    "rolstd = tempDiff.rolling(20).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "orig = plt.plot(tempDiff, color='blue', label='Original')\n",
    "mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "std = plt.plot(rolstd, color='black', label = 'Rolling Std Deviation')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.legend(loc='best')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(tempDiff, model='additive')\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Dickey-Fuller test\n",
    "res_DF = adfuller(tempDiff)\n",
    "print('ADF Statistic: %f' % res_DF[0])\n",
    "print('p-value: %f' % res_DF[1])\n",
    "print('Critical Values:')\n",
    "for key, value in res_DF[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation (acf) & Partial autocorrelation (pacf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "# the autocorrelation chart provides just the correlation at increasing lags\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "plot_acf(temp.values, lags=50, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "fig, ax = plt.subplots(figsize=(12,5)) \n",
    "plot_pacf(temp.values, lags=50, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "plot_acf(tempDiff.values, lags=50, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "plot_pacf(tempDiff.values, lags=50, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ARIMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a ARMA(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "arma = ARMA(tuple(tempDiff.values), (4,2)).fit()\n",
    "arma.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(tempDiff.values, color='blue')\n",
    "preds = arma.fittedvalues\n",
    "plt.plot(preds, color='red')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 12\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(tempDiff.values, color='blue')\n",
    "\n",
    "forecast = arma.forecast(steps=steps)[0]\n",
    "p0 = preds\n",
    "f0 =  forecast\n",
    "predForecast0= np.append(p0,f0)\n",
    "predForecast0 = pd.DataFrame(predForecast0)\n",
    "\n",
    "plt.plot(predForecast0, color='green')\n",
    "plt.plot(p0, color='red')\n",
    "plt.title('Display the predictions with the ARIMA model')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 12\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "axes = plt.axes()\n",
    "axes.set_xlim([2800,3170])\n",
    "\n",
    "plt.plot(tempDiff.values, color='blue')\n",
    "\n",
    "forecast = arma.forecast(steps=steps)[0]\n",
    "p0 = preds\n",
    "f0 =  forecast\n",
    "predForecast0= np.append(p0,f0)\n",
    "predForecast0 = pd.DataFrame(predForecast0)\n",
    "\n",
    "plt.plot(predForecast0, color='green')\n",
    "plt.plot(p0, color='red')\n",
    "plt.title('Display the predictions with the ARIMA model')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "tempLevel0 = np.append(startValue,tempDiff).cumsum()\n",
    "plt.plot(tempLevel0, color='blue')\n",
    "\n",
    "\n",
    "tempLevel_Forecast0 = np.append(startValue,predForecast0).cumsum()\n",
    "plt.plot(tempLevel_Forecast0,color='green')\n",
    "\n",
    "tempLevel_pred0 = np.append(startValue,p0).cumsum()\n",
    "plt.plot(tempLevel_pred0,color='red')\n",
    "plt.title('Display the predictions with the ARIMA model - with the level data')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtempLevel = pd.DataFrame (tempLevel0)\n",
    "xtempLevel.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "axes = plt.axes()\n",
    "axes.set_xlim([2800,3170])\n",
    "\n",
    "tempLevel0 = np.append(startValue,tempDiff).cumsum()\n",
    "plt.plot(tempLevel0, color='blue')\n",
    "\n",
    "\n",
    "tempLevel_Forecast0 = np.append(startValue,predForecast0).cumsum()\n",
    "plt.plot(tempLevel_Forecast0,color='green')\n",
    "\n",
    "tempLevel_pred0 = np.append(startValue,p0).cumsum()\n",
    "plt.plot(tempLevel_pred0,color='red')\n",
    "\n",
    "plt.title('Display the predictions with the ARIMA model - with the level data')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(tempLevel0, tempLevel_pred0)\n",
    "print('MAE',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "mape=mean_absolute_percentage_error(tempLevel0, tempLevel_pred0)\n",
    "print('MAPE %',mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train Test Split method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tempDiff.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20\n",
    "train_size = int(len(X) * (1-test_size))\n",
    "train, test = X[0:train_size], X[train_size:len(X)]\n",
    "print('Observations: %d' % (len(X)))\n",
    "print('Training Observations: %d' % (len(train)))\n",
    "print('Testing Observations: %d' % (len(test)))\n",
    "plt.plot(train)\n",
    "plt.plot([None for i in train] + [x for x in test])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Build ARIMA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACF & PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "plot_acf(train, lags=50, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "plot_pacf(train, lags=50, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "arma_v2 = ARMA(tuple(train), (4,2)).fit()\n",
    "arma_v2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(test)\n",
    "y_arma_forecast = arma_v2.forecast(steps=steps)[0]\n",
    "y_arma_forecast.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_arma = mean_absolute_error(test, y_arma_forecast)\n",
    "print('MAE ARMA : ',mae_arma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape_arma = mean_absolute_percentage_error(test, y_arma_forecast)\n",
    "print('MAPE ARMA %',mape_arma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(data, n_prev = 100):  \n",
    "    \"\"\"\n",
    "    data should be pd.DataFrame()\n",
    "    \"\"\"\n",
    "\n",
    "    docX, docY = [], []\n",
    "    for i in range(len(data)-n_prev):\n",
    "        docX.append(data.iloc[i:i+n_prev].to_numpy())\n",
    "        docY.append(data.iloc[i+n_prev].to_numpy())\n",
    " \n",
    "    alsX = np.array(docX)\n",
    "    alsY = np.array(docY)\n",
    "\n",
    "    return alsX, alsY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tempDiff)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_sequences = 10\n",
    "test_size = 0.20\n",
    "train_size = int(len(X) * (1-test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[[\"diff\"]].iloc[:train_size]\n",
    "df_test  = df[[\"diff\"]].iloc[train_size:]\n",
    "(X_train, y_train) = _load_data(df_train, n_prev = length_of_sequences)\n",
    "(X_test, y_test)   = _load_data(df_test, n_prev = length_of_sequences)  \n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"lstm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple RNN  \n",
    "https://www.programcreek.com/python/example/89700/keras.layers.SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(length_of_sequences, batch_size = None, stateful = False):\n",
    "    in_out_neurons = 1\n",
    "    hidden_neurons = 1\n",
    "    inp = Input(batch_shape=(batch_size, \n",
    "                length_of_sequences, \n",
    "                in_out_neurons))  \n",
    "\n",
    "    rnn = SimpleRNN(hidden_neurons, \n",
    "                    return_sequences=False,\n",
    "                    stateful = stateful,\n",
    "                    name=\"RNN\")(inp)\n",
    "\n",
    "    dens = Dense(in_out_neurons,name=\"dense\")(rnn)\n",
    "    model = Model(inputs=[inp],outputs=[dens])\n",
    "    \n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
    "\n",
    "    \n",
    "    return(model,(inp,rnn,dens))\n",
    "## use the default values for batch_size, stateful\n",
    "rnn_model, (inp,rnn,dens) = define_model(length_of_sequences = X_train.shape[1])\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = rnn_model.fit(X_train, y_train, batch_size=600, epochs=1000, \n",
    "                 verbose=False,validation_split=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"loss\",\"val_loss\"]:\n",
    "    plt.plot(hist.history[label],label=label)\n",
    "\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"The final validation loss: {}\".format(hist.history[\"val_loss\"][-1]))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rnn_predictions = rnn_model.predict(X_test)\n",
    "y_rnn_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_rnn = mean_absolute_error(y_test, y_rnn_predictions)\n",
    "print('MAE RNN : ',mae_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape_rnn = mean_absolute_percentage_error(y_test, y_rnn_predictions)\n",
    "print('MAPE RNN %',mape_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://fairyonice.github.io/Understand-Keras%27s-RNN-behind-the-scenes-with-a-sin-wave-example.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "n_input= X_train.shape[1]\n",
    "n_features = X_train.shape[2] \n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(10, activation='tanh', input_shape=(n_input, n_features)))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " history_lstm_model = lstm_model.fit(X_train, y_train, epochs=50, verbose = False ,validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"loss\",\"val_loss\"]:\n",
    "    plt.plot(history_lstm_model.history[label],label=label)\n",
    "\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"The final validation loss: {}\".format(history_lstm_model.history[\"val_loss\"][-1]))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on y_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lstm_predictions = lstm_model.predict(X_test)\n",
    "y_lstm_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_lstm = mean_absolute_error(y_test, y_lstm_predictions)\n",
    "print('MAE LSTM : ',mae_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape_lstm = mean_absolute_percentage_error(y_test, y_lstm_predictions)\n",
    "print('MAPE LSTM %',mape_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_results = pd.DataFrame({'ARMA' : [mae_arma, mape_arma],\n",
    "                               'RNN (KERAS)' : [mae_rnn, mape_rnn],\n",
    "                               'LSTM (KERAS)' : [mae_lstm, mape_lstm]\n",
    "                              }, index = ['MAE', 'MAPE (%)']\n",
    "                              )\n",
    "global_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "251.583px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
